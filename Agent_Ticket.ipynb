{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63756560-324d-4b02-868d-5e90b8f9fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: âœ… Using Apple Silicon GPU (MPS)\n",
      "INFO: ðŸ“Š Device: mps\n",
      "INFO: ðŸ”§ PyTorch version: 2.8.0\n",
      "INFO: ðŸŽ MPS built: True\n",
      "INFO: ðŸ”„ Initializing local embedding model...\n",
      "INFO: ðŸ“¦ Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO: ðŸ’» Device: mps\n",
      "INFO: ðŸŽ MPS fallback enabled for unsupported operations\n",
      "INFO: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO: âœ… Local embedding model loaded successfully\n",
      "INFO: ðŸŽ¯ No API calls - fully offline embeddings\n",
      "INFO: âœ… Embedding test successful! Dimension: 384\n",
      "E0000 00:00:1760100288.589762 21847708 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760100288.603345 21847708 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "INFO: âœ… LLMs initialized:\n",
      "E0000 00:00:1760100288.604636 21847708 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "INFO:    Preprocessor: gemini-2.5-pro\n",
      "INFO:    Classifier: gemini-2.5-flash-preview-09-2025\n",
      "INFO:    Resolver: gemini-2.5-flash-preview-09-2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GST Grievance Resolution System Loaded!\n",
      "\n",
      "ðŸ“š System Features:\n",
      "   â€¢ Local embeddings (MPS/CUDA accelerated)\n",
      "   â€¢ Persistent FAISS vector store\n",
      "   â€¢ Web search (Tavily/DuckDuckGo)\n",
      "   â€¢ Twitter real-time updates\n",
      "\n",
      "ðŸ’¡ Usage:\n",
      "   result = process_gst_grievance(\"Your GST query here\")\n",
      "   display_result(result)\n",
      "\n",
      "ðŸ”§ Utilities:\n",
      "   stats = get_kb_stats()  # View knowledge base stats\n",
      "   add_to_knowledge_base([{...}])  # Add documents\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GST GRIEVANCE RESOLUTION MULTI-AGENT SYSTEM (ENHANCED VERSION)\n",
    "# ============================================================================\n",
    "# Architecture: Multi-agent system with LangGraph orchestration\n",
    "# Primary LLM: Gemini 2.5 Pro for resolution\n",
    "# Supporting LLM: Gemini 2.5 Flash for preprocessing and classification\n",
    "# Embeddings: LOCAL sentence-transformers (NO API CALLS)\n",
    "# Storage: Persistent FAISS vector database\n",
    "# Web Search: Tavily API / DuckDuckGo fallback\n",
    "# ============================================================================\n",
    "\n",
    "# --- IMPORTS ---\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, TypedDict, Annotated\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# PyTorch - MUST IMPORT BEFORE Config class\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Vector store and embeddings - LOCAL MODELS ONLY\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "## Knowledge Graph\n",
    "import sqlite3\n",
    "import json\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Web search providers\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    TAVILY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TAVILY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from langchain_community.tools import DuckDuckGoSearchRun\n",
    "    from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "    DUCKDUCKGO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DUCKDUCKGO_AVAILABLE = False\n",
    "\n",
    "# Twitter API (optional)\n",
    "try:\n",
    "    import tweepy\n",
    "except ImportError:\n",
    "    tweepy = None\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "class Config:\n",
    "    \"\"\"Configuration management\"\"\"\n",
    "    # API Keys\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    TWITTER_BEARER_TOKEN = os.getenv(\"TWITTER_BEARER_TOKEN\")\n",
    "    TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "    DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "    \n",
    "    # Model configurations\n",
    "    PREPROCESSOR_MODEL = \"gemini-2.5-pro\"\n",
    "    CLASSIFIER_MODEL = \"gemini-2.5-flash-preview-09-2025\"\n",
    "    RESOLVER_MODEL = \"gemini-2.5-flash-preview-09-2025\"\n",
    "    DEEPSEEK_MODEL = \"deepseek-chat\" \n",
    "    \n",
    "    # Embedding model configuration\n",
    "    LOCAL_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # Device detection for macOS M1 (MPS), CUDA, or CPU\n",
    "    @staticmethod\n",
    "    def get_device():\n",
    "        \"\"\"Auto-detect best available device\"\"\"\n",
    "        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            logger.info(\"âœ… Using Apple Silicon GPU (MPS)\")\n",
    "            return \"mps\"\n",
    "        elif torch.cuda.is_available():\n",
    "            logger.info(\"âœ… Using NVIDIA CUDA GPU\")\n",
    "            return \"cuda\"\n",
    "        else:\n",
    "            logger.info(\"â„¹ï¸ Using CPU (no GPU detected)\")\n",
    "            return \"cpu\"\n",
    "    \n",
    "    EMBEDDING_DEVICE = get_device.__func__()\n",
    "    \n",
    "    # Storage paths\n",
    "    VECTOR_STORE_PATH = \"./data/gst_knowledge_base\"\n",
    "    \n",
    "    # Thresholds and configurations\n",
    "    MIN_CONFIDENCE_THRESHOLD = 95\n",
    "    NULL_RESPONSE_THRESHOLD = 95\n",
    "    PREPROCESSOR_TEMPERATURE = 0.2\n",
    "    CLASSIFIER_TEMPERATURE = 0.2\n",
    "    RESOLVER_TEMPERATURE = 0.0\n",
    "    MAX_RETRIEVAL_TIME = 10\n",
    "    \n",
    "    # Retrieval settings\n",
    "    MAX_LOCAL_RESULTS = 5\n",
    "    MAX_WEB_RESULTS = 10\n",
    "    MAX_TWITTER_RESULTS = 10\n",
    "\n",
    "    # Web search settings \n",
    "    WEB_SEARCH_UNRESTRICTED = True  # Set False to use domain filtering\n",
    "    \n",
    "    # GST specific URLs\n",
    "    GSTN_FAQ_URL = \"https://tutorial.gst.gov.in/downloads/news/FAQ.pdf\"\n",
    "    CBIC_BASE_URL = \"https://cbic-gst.gov.in\"\n",
    "    GSTN_TWITTER = \"@Infosys_GSTN\"\n",
    "    \n",
    "\n",
    "# Validate configuration\n",
    "if not Config.GOOGLE_API_KEY:\n",
    "    logger.warning(\"âš ï¸ WARNING: GOOGLE_API_KEY not found in .env file\")\n",
    "\n",
    "# Log device information\n",
    "logger.info(f\"ðŸ“Š Device: {Config.EMBEDDING_DEVICE}\")\n",
    "logger.info(f\"ðŸ”§ PyTorch version: {torch.__version__}\")\n",
    "if Config.EMBEDDING_DEVICE == \"mps\":\n",
    "    logger.info(f\"ðŸŽ MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# --- ENUMS AND MODELS ---\n",
    "class GrievanceCategory(str, Enum):\n",
    "    REGISTRATION = \"registration\"\n",
    "    GSTR_FILING = \"gstr_filing\"\n",
    "    EWAY_BILL = \"eway_bill\"\n",
    "    REFUND = \"refund\"\n",
    "    ITC_MISMATCH = \"itc_mismatch\"\n",
    "    PENALTY_NOTICE = \"penalty_notice\"\n",
    "    PORTAL_ERROR = \"portal_error\"\n",
    "    API_INTEGRATION = \"api_integration\"\n",
    "    COMPLIANCE = \"compliance\"\n",
    "    GENERAL = \"general\"\n",
    "\n",
    "class IntentType(str, Enum):\n",
    "    INFORMATIONAL = \"informational\"\n",
    "    PROCEDURAL = \"procedural\"\n",
    "    ERROR_RESOLUTION = \"error_resolution\"\n",
    "    COMPLIANCE_CLARIFICATION = \"compliance_clarification\"\n",
    "    REFUND_STATUS = \"refund_status\"\n",
    "\n",
    "class ExtractedEntity(BaseModel):\n",
    "    entity_type: str\n",
    "    value: str\n",
    "    context: Optional[str] = None\n",
    "\n",
    "class CoreIssue(BaseModel):\n",
    "    issue_text: str\n",
    "    keywords: List[str]\n",
    "    priority: int\n",
    "\n",
    "class PreprocessingOutput(BaseModel):\n",
    "    cleaned_text: str\n",
    "    detected_intent: str\n",
    "    core_issues: List[CoreIssue]\n",
    "    entities: List[ExtractedEntity]\n",
    "    language: str\n",
    "\n",
    "class ClassificationOutput(BaseModel):\n",
    "    primary_category: str\n",
    "    secondary_categories: List[str]\n",
    "    confidence_scores: Dict[str, float]\n",
    "    sub_type: Optional[str] = None\n",
    "\n",
    "class RetrievalSource(BaseModel):\n",
    "    source_type: str\n",
    "    content: str\n",
    "    citation: str\n",
    "    relevance_score: float\n",
    "    date: Optional[str] = None\n",
    "\n",
    "class RetrievalOutput(BaseModel):\n",
    "    twitter_results: List[RetrievalSource]\n",
    "    local_results: List[RetrievalSource] \n",
    "    web_results: List[RetrievalSource]  \n",
    "    llm_reasoning: List[RetrievalSource]\n",
    "    total_sources: int\n",
    "    retrieval_time: float\n",
    "\n",
    "class IssueResolution(BaseModel):\n",
    "    issue: str\n",
    "    resolution: Optional[str]\n",
    "    confidence: int\n",
    "    legal_basis: Optional[str] = None\n",
    "    procedural_steps: Optional[List[str]] = None\n",
    "    source_citations: List[str]\n",
    "    reason_for_null: Optional[str] = None\n",
    "\n",
    "class ResolverOutput(BaseModel):\n",
    "    resolutions: List[IssueResolution]\n",
    "    overall_confidence: int\n",
    "    requires_escalation: bool\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    direct_answer: str\n",
    "    detailed_explanation: Optional[str] = None\n",
    "    legal_basis: Optional[str] = None\n",
    "    recent_updates: Optional[str] = None\n",
    "    additional_resources: List[str]\n",
    "    confidence_score: int\n",
    "    requires_manual_review: bool\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_query: str\n",
    "    session_id: str\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    preprocessing_output: Optional[PreprocessingOutput]\n",
    "    classification_output: Optional[ClassificationOutput]\n",
    "    retrieval_output: Optional[RetrievalOutput]\n",
    "    resolver_output: Optional[ResolverOutput]\n",
    "    final_response: Optional[FinalResponse]\n",
    "    timestamp: str\n",
    "    processing_time: float\n",
    "    errors: List[str]\n",
    "    feedback_received: Optional[str]\n",
    "    iteration_count: int\n",
    "    escalation_requested: bool\n",
    "\n",
    "class LightweightKnowledgeGraph:\n",
    "    \"\"\"Embedded knowledge graph retriever\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"./knowledge_graph.db\"):\n",
    "        self.db_path = Path(db_path)\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.conn = None\n",
    "        \n",
    "        if self.db_path.exists():\n",
    "            self.conn = sqlite3.connect(str(self.db_path))\n",
    "        else:\n",
    "            logger.warning(f\"âš ï¸ Graph database not found: {db_path}\")\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load graph from SQLite database\"\"\"\n",
    "        if not self.conn:\n",
    "            logger.error(\"âŒ No database connection for knowledge graph\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Load nodes - FIXED: Avoid 'type' keyword conflict\n",
    "            cursor = self.conn.execute(\"SELECT id, type, label, metadata FROM nodes\")\n",
    "            for node_id, node_type, label, metadata in cursor:\n",
    "                meta = json.loads(metadata) if metadata else {}\n",
    "                meta['entity_type'] = node_type  # Store as 'entity_type' instead of 'type'\n",
    "                meta['label'] = label\n",
    "                self.graph.add_node(node_id, **meta)\n",
    "            \n",
    "            # Load edges\n",
    "            cursor = self.conn.execute(\"SELECT source, target, relation, weight FROM edges\")\n",
    "            for source, target, relation, weight in cursor:\n",
    "                self.graph.add_edge(source, target, relation=relation, weight=weight)\n",
    "            \n",
    "            logger.info(f\"âœ… Graph loaded: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Failed to load graph: {e}\")\n",
    "    \n",
    "    def find_related(self, entity: str, max_depth: int = 2, max_results: int = 20) -> list:\n",
    "        \"\"\"Find entities related to entity via BFS traversal\"\"\"\n",
    "        if entity not in self.graph:\n",
    "            return []\n",
    "        \n",
    "        related = []\n",
    "        visited = set()\n",
    "        queue = [(entity, 0)]\n",
    "        \n",
    "        while queue and len(related) < max_results:\n",
    "            node, depth = queue.pop(0)\n",
    "            if depth > max_depth or node in visited:\n",
    "                continue\n",
    "            visited.add(node)\n",
    "            \n",
    "            if node != entity and not node.startswith(\"doc:\"):\n",
    "                related.append(node)\n",
    "            \n",
    "            if depth < max_depth:\n",
    "                for neighbor in self.graph.neighbors(node):\n",
    "                    if neighbor not in visited:\n",
    "                        queue.append((neighbor, depth + 1))\n",
    "        \n",
    "        return related[:max_results]\n",
    "    \n",
    "    def get_entity_info(self, entity: str) -> dict:\n",
    "        \"\"\"Return node metadata for an entity\"\"\"\n",
    "        if entity not in self.graph:\n",
    "            return {}\n",
    "        return dict(self.graph.nodes[entity])\n",
    "    \n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "# --- INITIALIZE LOCAL EMBEDDINGS ---\n",
    "def initialize_local_embeddings():\n",
    "    \"\"\"Initialize local embedding model - NO API CALLS\"\"\"\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Initializing local embedding model...\")\n",
    "        logger.info(f\"ðŸ“¦ Model: {Config.LOCAL_EMBEDDING_MODEL}\")\n",
    "        logger.info(f\"ðŸ’» Device: {Config.EMBEDDING_DEVICE}\")\n",
    "        \n",
    "        if Config.EMBEDDING_DEVICE == \"mps\":\n",
    "            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "            logger.info(\"ðŸŽ MPS fallback enabled for unsupported operations\")\n",
    "        \n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=Config.LOCAL_EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': Config.EMBEDDING_DEVICE},\n",
    "            encode_kwargs={'normalize_embeddings': True, 'batch_size': 32},\n",
    "            show_progress=False\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"âœ… Local embedding model loaded successfully\")\n",
    "        logger.info(f\"ðŸŽ¯ No API calls - fully offline embeddings\")\n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to initialize local embeddings: {e}\")\n",
    "        raise\n",
    "        \n",
    "# Initialize global embeddings instance\n",
    "try:\n",
    "    local_embeddings = initialize_local_embeddings()\n",
    "    test_embedding = local_embeddings.embed_query(\"Test query for GST portal\")\n",
    "    logger.info(f\"âœ… Embedding test successful! Dimension: {len(test_embedding)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"âŒ Embedding initialization failed: {e}\")\n",
    "    local_embeddings = None\n",
    "    \n",
    "# --- LLM INITIALIZATION ---\n",
    "def initialize_llms():\n",
    "    \"\"\"Initialize LLM instances with separate preprocessor\"\"\"\n",
    "    \n",
    "    preprocessor_llm = ChatGoogleGenerativeAI(\n",
    "        model=Config.PREPROCESSOR_MODEL,  # gemini-2.5-pro\n",
    "        temperature=Config.PREPROCESSOR_TEMPERATURE,\n",
    "        google_api_key=Config.GOOGLE_API_KEY,\n",
    "        response_mime_type=\"application/json\"\n",
    "    )\n",
    "    \n",
    "    classifier_llm = ChatGoogleGenerativeAI(\n",
    "        model=Config.CLASSIFIER_MODEL,\n",
    "        temperature=Config.CLASSIFIER_TEMPERATURE,\n",
    "        google_api_key=Config.GOOGLE_API_KEY,\n",
    "        response_mime_type=\"application/json\"\n",
    "    )\n",
    "\n",
    "    resolver_llm = ChatGoogleGenerativeAI(\n",
    "        model=Config.RESOLVER_MODEL,\n",
    "        temperature=Config.RESOLVER_TEMPERATURE,\n",
    "        google_api_key=Config.GOOGLE_API_KEY,\n",
    "        response_mime_type=\"application/json\"\n",
    "    )\n",
    "\n",
    "    logger.info(f\"âœ… LLMs initialized:\")\n",
    "    logger.info(f\"   Preprocessor: {Config.PREPROCESSOR_MODEL}\")\n",
    "    logger.info(f\"   Classifier: {Config.CLASSIFIER_MODEL}\")\n",
    "    logger.info(f\"   Resolver: {Config.RESOLVER_MODEL}\")\n",
    "    return preprocessor_llm, classifier_llm, resolver_llm\n",
    "\n",
    "preprocessor_llm, classifier_llm, resolver_llm = initialize_llms()\n",
    "\n",
    "# --- PREPROCESSING & CLASSIFICATION AGENTS ---\n",
    "class PreprocessingAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.parser = JsonOutputParser(pydantic_object=PreprocessingOutput)\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert at preprocessing GST-related grievance queries.\n",
    "\n",
    "**User Query:** {query}\n",
    "\n",
    "Extract: intent, core issues, entities (GSTIN, GST forms like GSTR-3A/GSTR-4, dates, amounts), language.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "    def process(self, state: AgentState) -> AgentState:\n",
    "        try:\n",
    "            logger.info(\"ðŸ”„ Agent 1: Preprocessing...\")\n",
    "            response = self.llm.invoke(self.prompt.format(\n",
    "                query=state[\"user_query\"],\n",
    "                format_instructions=self.parser.get_format_instructions()\n",
    "            ))\n",
    "            preprocessing_output = PreprocessingOutput(**self.parser.parse(response.content))\n",
    "            state[\"preprocessing_output\"] = preprocessing_output\n",
    "            logger.info(f\"âœ… Found {len(preprocessing_output.core_issues)} issues\")\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Preprocessing error: {e}\")\n",
    "            state[\"errors\"].append(str(e))\n",
    "            return state\n",
    "\n",
    "class ClassificationAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.parser = JsonOutputParser(pydantic_object=ClassificationOutput)\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Classify GST grievance. Categories: registration, gstr_filing, penalty_notice, portal_error, refund, itc_mismatch, eway_bill, etc.\n",
    "\n",
    "Intent: {intent}\n",
    "Issues: {core_issues}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "    def process(self, state: AgentState) -> AgentState:\n",
    "        try:\n",
    "            logger.info(\"ðŸ”„ Agent 2: Classifying...\")\n",
    "            preprocessing = state[\"preprocessing_output\"]\n",
    "            response = self.llm.invoke(self.prompt.format(\n",
    "                intent=preprocessing.detected_intent,\n",
    "                core_issues=json.dumps([i.model_dump() for i in preprocessing.core_issues]),\n",
    "                format_instructions=self.parser.get_format_instructions()\n",
    "            ))\n",
    "            classification_output = ClassificationOutput(**self.parser.parse(response.content))\n",
    "            state[\"classification_output\"] = classification_output\n",
    "            logger.info(f\"âœ… Primary: {classification_output.primary_category}\")\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Classification error: {e}\")\n",
    "            state[\"errors\"].append(str(e))\n",
    "            return state\n",
    "\n",
    "# --- NEW: LOCAL RETRIEVAL AGENT (Persistent FAISS) ---\n",
    "class LocalRetrievalAgent:\n",
    "    \"\"\"\n",
    "    PRODUCTION: Load pre-built knowledge base with graph support\n",
    "    Handles large-scale PDF ingestion output (20K+ chunks)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings=None, kb_folder: str = \"./\", enable_graph: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize with pre-built knowledge base\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Pre-initialized local embeddings (must match ingestion model)\n",
    "            kb_folder: Path to processed knowledge base folder (contains faiss_index/, kb_metadata.json, etc.)\n",
    "            enable_graph: Enable knowledge graph retrieval for entity relationships\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings or local_embeddings\n",
    "        self.kb_folder = Path(kb_folder)\n",
    "        self.enable_graph = enable_graph\n",
    "        self.vector_store = None\n",
    "        self.kb_metadata = {}\n",
    "        self.graph_retriever = None\n",
    "        \n",
    "        if not self.embeddings:\n",
    "            logger.error(\"âŒ No local embeddings provided!\")\n",
    "            return\n",
    "        \n",
    "        # Load pre-built knowledge base\n",
    "        self._load_knowledge_base()\n",
    "        \n",
    "        # Load knowledge graph if enabled\n",
    "        if self.enable_graph:\n",
    "            self._load_knowledge_graph()\n",
    "    \n",
    "    def _load_knowledge_base(self):\n",
    "        \"\"\"Load pre-processed knowledge base from disk\"\"\"\n",
    "        faiss_path = self.kb_folder / \"faiss_index\"\n",
    "        metadata_path = self.kb_folder / \"kb_metadata.json\"\n",
    "        \n",
    "        if not faiss_path.exists():\n",
    "            logger.error(f\"âŒ No processed knowledge base found at {faiss_path}\")\n",
    "            logger.info(\"ðŸ’¡ Expected structure:\")\n",
    "            logger.info(\"   ./faiss_index/ - FAISS vector store\")\n",
    "            logger.info(\"   ./kb_metadata.json - Metadata\")\n",
    "            logger.info(\"   ./knowledge_graph.db - Graph database\")\n",
    "            logger.info(\"\\nðŸ’¡ Run your PDF ingestion pipeline first!\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"ðŸ”„ Loading knowledge base from {self.kb_folder}\")\n",
    "            \n",
    "            # Load FAISS index\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                str(faiss_path),\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            \n",
    "            # Load metadata\n",
    "            if metadata_path.exists():\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    self.kb_metadata = json.load(f)\n",
    "            \n",
    "            # Log stats\n",
    "            total_docs = self.kb_metadata.get('total_files', self.kb_metadata.get('total_pdfs', 'Unknown'))\n",
    "            total_chunks = self.kb_metadata.get('total_chunks', 'Unknown')\n",
    "            embedding_model = self.kb_metadata.get('embedding_model', 'Unknown')\n",
    "            \n",
    "            logger.info(f\"âœ… Loaded knowledge base:\")\n",
    "            logger.info(f\"   ðŸ“„ Documents: {total_docs}\")\n",
    "            logger.info(f\"   ðŸ“Š Chunks: {total_chunks}\")\n",
    "            logger.info(f\"   ðŸ§  Model: {embedding_model}\")\n",
    "            logger.info(f\"   ðŸ’¾ Size: {self._get_index_size():.2f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Failed to load knowledge base: {e}\")\n",
    "            logger.error(f\"   Check that embedding model matches: {Config.LOCAL_EMBEDDING_MODEL}\")\n",
    "            self.vector_store = None\n",
    "    \n",
    "    def _load_knowledge_graph(self):\n",
    "        \"\"\"Load knowledge graph for relationship-based retrieval\"\"\"\n",
    "        graph_path = self.kb_folder / \"knowledge_graph.db\"\n",
    "        \n",
    "        if not graph_path.exists():\n",
    "            logger.warning(f\"âš ï¸ Knowledge graph not found at {graph_path}\")\n",
    "            logger.info(\"   Graph-enhanced retrieval disabled\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Import graph retriever            \n",
    "            self.graph_retriever = LightweightKnowledgeGraph(db_path=str(graph_path))\n",
    "            self.graph_retriever.load()\n",
    "            logger.info(f\"âœ… Loaded knowledge graph:\")\n",
    "            logger.info(f\"   ðŸ•¸ï¸ Nodes: {self.graph_retriever.graph.number_of_nodes()}\")\n",
    "            logger.info(f\"   ðŸ”— Edges: {self.graph_retriever.graph.number_of_edges()}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.warning(\"âš ï¸ Knowledge graph module not found. Skipping graph loading.\")\n",
    "            self.graph_retriever = None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Failed to load knowledge graph: {e}\")\n",
    "            self.graph_retriever = None\n",
    "    \n",
    "    def _get_index_size(self) -> float:\n",
    "        \"\"\"Calculate total size of knowledge base in MB\"\"\"\n",
    "        if not self.kb_folder.exists():\n",
    "            return 0.0\n",
    "        total_bytes = sum(f.stat().st_size for f in self.kb_folder.rglob('*') if f.is_file())\n",
    "        return total_bytes / (1024 * 1024)\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 5, filter_category: Optional[str] = None, \n",
    "                 use_graph: bool = True) -> List[RetrievalSource]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks using hybrid vector + graph search\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results\n",
    "            filter_category: Optional category filter\n",
    "            use_graph: Enable graph-enhanced retrieval (if available)\n",
    "        \n",
    "        Returns:\n",
    "            List of RetrievalSource objects\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        if not self.vector_store:\n",
    "            logger.warning(\"âš ï¸ Vector store not available\")\n",
    "            return results\n",
    "        \n",
    "        try:\n",
    "            # --- PHASE 1: VECTOR SEARCH ---\n",
    "            # Fetch more than k for filtering and re-ranking\n",
    "            fetch_k = k * 3 if use_graph and self.graph_retriever else k * 2\n",
    "            \n",
    "            docs = self.vector_store.similarity_search_with_score(query, k=fetch_k)\n",
    "            \n",
    "            vector_results = []\n",
    "            for doc, score in docs:\n",
    "                # Apply category filter if specified\n",
    "                if filter_category and doc.metadata.get('category') != filter_category:\n",
    "                    continue\n",
    "                \n",
    "                # Build citation from metadata\n",
    "                filename = doc.metadata.get('filename', doc.metadata.get('source', 'Unknown'))\n",
    "                page_info = doc.metadata.get('page_num', doc.metadata.get('slide_num', ''))\n",
    "                \n",
    "                if page_info:\n",
    "                    citation = f\"{filename} (Page {page_info})\"\n",
    "                else:\n",
    "                    citation = filename\n",
    "                \n",
    "                vector_results.append({\n",
    "                    'source': RetrievalSource(\n",
    "                        source_type=\"local_kb\",\n",
    "                        content=doc.page_content,\n",
    "                        citation=citation,\n",
    "                        relevance_score=float(1 - score),  # Convert distance to similarity\n",
    "                        date=None\n",
    "                    ),\n",
    "                    'metadata': doc.metadata,\n",
    "                    'vector_score': float(1 - score)\n",
    "                })\n",
    "            \n",
    "            # --- PHASE 2: GRAPH ENHANCEMENT (Optional) ---\n",
    "            if use_graph and self.graph_retriever:\n",
    "                graph_boost = self._apply_graph_boost(query, vector_results)\n",
    "                vector_results = graph_boost\n",
    "            \n",
    "            # --- PHASE 3: RETURN TOP-K ---\n",
    "            # Sort by final score and take top k\n",
    "            vector_results.sort(key=lambda x: x['vector_score'], reverse=True)\n",
    "            \n",
    "            for item in vector_results[:k]:\n",
    "                results.append(item['source'])\n",
    "            \n",
    "            logger.info(f\"   ðŸ“– Retrieved {len(results)} chunks from knowledge base\")\n",
    "            if use_graph and self.graph_retriever:\n",
    "                logger.info(f\"      ðŸ•¸ï¸ Graph-enhanced retrieval active\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Retrieval error: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _apply_graph_boost(self, query: str, vector_results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Boost relevance scores using knowledge graph relationships\n",
    "        \n",
    "        Strategy:\n",
    "        1. Extract entities from query\n",
    "        2. Find related entities in graph\n",
    "        3. Boost chunks that mention related entities\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract entities from query (simple regex for now)\n",
    "            import re\n",
    "            \n",
    "            query_entities = set()\n",
    "            \n",
    "            # GST Forms\n",
    "            forms = re.findall(r'GSTR-?\\d+[A-Z]?', query, re.IGNORECASE)\n",
    "            query_entities.update([f\"forms:{f}\" for f in forms])\n",
    "            \n",
    "            # Sections\n",
    "            sections = re.findall(r'Section\\s+\\d+[A-Z]?', query, re.IGNORECASE)\n",
    "            query_entities.update([f\"sections:{s}\" for s in sections])\n",
    "            \n",
    "            # Notifications\n",
    "            notifications = re.findall(r'Notification\\s+(?:No\\.?\\s*)?\\d+/\\d{4}', query, re.IGNORECASE)\n",
    "            query_entities.update([f\"notifications:{n}\" for n in notifications])\n",
    "            \n",
    "            if not query_entities:\n",
    "                return vector_results  # No boost if no entities found\n",
    "            \n",
    "            # Find related entities via graph\n",
    "            related_entities = set()\n",
    "            for entity in query_entities:\n",
    "                if entity in self.graph_retriever.graph:\n",
    "                    related = self.graph_retriever.find_related(entity, max_depth=2)\n",
    "                    related_entities.update(related)\n",
    "            \n",
    "            # Boost chunks that mention related entities\n",
    "            for result in vector_results:\n",
    "                content = result['source'].content.lower()\n",
    "                boost = 0.0\n",
    "                \n",
    "                for related_entity in related_entities:\n",
    "                    # Extract entity label (e.g., \"forms:GSTR-4\" -> \"GSTR-4\")\n",
    "                    entity_label = related_entity.split(\":\", 1)[-1].lower()\n",
    "                    if entity_label in content:\n",
    "                        boost += 0.05  # Small boost per related entity\n",
    "                \n",
    "                # Apply boost (cap at +0.2)\n",
    "                result['vector_score'] = min(result['vector_score'] + min(boost, 0.2), 1.0)\n",
    "            \n",
    "            logger.info(f\"      ðŸŽ¯ Graph boost applied ({len(related_entities)} related entities)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"âš ï¸ Graph boost failed: {e}\")\n",
    "        \n",
    "        return vector_results\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get comprehensive statistics about the knowledge base\"\"\"\n",
    "        stats = {\n",
    "            \"document_count\": self.kb_metadata.get('total_chunks', 0),\n",
    "            \"source_files\": self.kb_metadata.get('total_files', self.kb_metadata.get('total_pdfs', 0)),\n",
    "            \"embedding_model\": self.kb_metadata.get('embedding_model', 'Unknown'),\n",
    "            \"chunk_size\": self.kb_metadata.get('chunk_size', 'Unknown'),\n",
    "            \"kb_folder\": str(self.kb_folder),\n",
    "            \"index_size_mb\": self._get_index_size(),\n",
    "            \"graph_enabled\": self.graph_retriever is not None\n",
    "        }\n",
    "        \n",
    "        if self.graph_retriever:\n",
    "            stats.update({\n",
    "                \"graph_nodes\": self.graph_retriever.graph.number_of_nodes(),\n",
    "                \"graph_edges\": self.graph_retriever.graph.number_of_edges()\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def search_with_graph(self, entity: str, max_related: int = 10) -> List[str]:\n",
    "        \"\"\"\n",
    "        Direct graph search: Find entities related to a given entity\n",
    "        \n",
    "        Args:\n",
    "            entity: Entity to search (e.g., \"forms:GSTR-4\")\n",
    "            max_related: Maximum number of related entities to return\n",
    "        \n",
    "        Returns:\n",
    "            List of related entity IDs\n",
    "        \"\"\"\n",
    "        if not self.graph_retriever:\n",
    "            logger.warning(\"âš ï¸ Knowledge graph not loaded\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            related = self.graph_retriever.find_related(entity, max_depth=2)\n",
    "            return related[:max_related]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Graph search error: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "# --- NEW: WEB RETRIEVAL AGENT (Internet Search) ---\n",
    "# --- NEW: WEB RETRIEVAL AGENT (Enhanced with LLM Query Optimization) ---\n",
    "class WebRetrievalAgent:\n",
    "    \"\"\"\n",
    "    Agent for web-based retrieval using internet search\n",
    "    Features:\n",
    "    - Tavily API (recommended, LLM-optimized)\n",
    "    - DuckDuckGo (free fallback)\n",
    "    - LLM-based query optimization for long queries\n",
    "    - Automatic fallback to regex extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = \"auto\", unrestricted: bool = True, llm=None):\n",
    "        \"\"\"\n",
    "        Initialize web retrieval agent\n",
    "        \n",
    "        Args:\n",
    "            provider: \"tavily\", \"duckduckgo\", or \"auto\"\n",
    "            unrestricted: True for general web search, False for domain filtering\n",
    "            llm: Optional LLM for query optimization (reuses classifier_llm)\n",
    "        \"\"\"\n",
    "        self.provider = None\n",
    "        self.client = None\n",
    "        self.unrestricted = unrestricted\n",
    "        self.llm = llm\n",
    "        \n",
    "        if provider == \"auto\":\n",
    "            self.provider = self._auto_detect_provider()\n",
    "        else:\n",
    "            self.provider = provider\n",
    "        \n",
    "        self._initialize_provider()\n",
    "    \n",
    "    def _auto_detect_provider(self) -> str:\n",
    "        \"\"\"Auto-detect available search provider\"\"\"\n",
    "        if TAVILY_AVAILABLE and Config.TAVILY_API_KEY:\n",
    "            logger.info(\"âœ… Tavily API key found, using Tavily\")\n",
    "            return \"tavily\"\n",
    "        \n",
    "        if DUCKDUCKGO_AVAILABLE:\n",
    "            logger.info(\"âœ… Using DuckDuckGo (free, no API key)\")\n",
    "            return \"duckduckgo\"\n",
    "        \n",
    "        logger.warning(\"âš ï¸ No search provider available - install tavily-python or duckduckgo-search\")\n",
    "        return None\n",
    "    \n",
    "    def _initialize_provider(self):\n",
    "        \"\"\"Initialize the selected search provider\"\"\"\n",
    "        if self.provider == \"tavily\":\n",
    "            self._initialize_tavily()\n",
    "        elif self.provider == \"duckduckgo\":\n",
    "            self._initialize_duckduckgo()\n",
    "    \n",
    "    def _initialize_tavily(self):\n",
    "        \"\"\"Initialize Tavily client\"\"\"\n",
    "        try:\n",
    "            self.client = TavilyClient(api_key=Config.TAVILY_API_KEY)\n",
    "            logger.info(\"âœ… Tavily client initialized\")\n",
    "            if self.unrestricted:\n",
    "                logger.info(\"ðŸŒ Unrestricted web search enabled (no domain filtering)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Tavily initialization failed: {e}\")\n",
    "            logger.info(\"ðŸ’¡ Get API key at: https://app.tavily.com/\")\n",
    "            self.client = None\n",
    "    \n",
    "    def _initialize_duckduckgo(self):\n",
    "        \"\"\"Initialize DuckDuckGo search\"\"\"\n",
    "        try:\n",
    "            wrapper = DuckDuckGoSearchAPIWrapper(\n",
    "                region=\"en-in\",\n",
    "                time=\"y\",\n",
    "                max_results=15,\n",
    "                safesearch=\"moderate\"\n",
    "            )\n",
    "            self.client = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "            logger.info(\"âœ… DuckDuckGo search initialized\")\n",
    "            if self.unrestricted:\n",
    "                logger.info(\"ðŸŒ Unrestricted web search enabled (general internet search)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ DuckDuckGo initialization failed: {e}\")\n",
    "            logger.info(\"ðŸ’¡ Install with: pip install duckduckgo-search\")\n",
    "            self.client = None\n",
    "    \n",
    "    # --- QUERY OPTIMIZATION METHODS ---\n",
    "    \n",
    "    def _extract_key_terms(self, text: str, max_terms: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract key terms from text using regex (fallback method)\n",
    "        \n",
    "        Priority:\n",
    "        1. GST form numbers (GSTR-4, GSTR-3A, etc.)\n",
    "        2. Notification numbers\n",
    "        3. Important keywords\n",
    "        4. Financial years\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        key_terms = []\n",
    "        \n",
    "        # 1. GST Forms (highest priority)\n",
    "        forms = re.findall(r'GSTR-?\\d+[A-Z]?', text, re.IGNORECASE)\n",
    "        key_terms.extend(forms[:3])\n",
    "        \n",
    "        # 2. Notification numbers\n",
    "        notifications = re.findall(r'Notification\\s+(?:No\\.?\\s*)?\\d+/\\d{4}', text, re.IGNORECASE)\n",
    "        key_terms.extend(notifications[:2])\n",
    "        \n",
    "        # 3. Important keywords\n",
    "        important_keywords = [\n",
    "            'late fee', 'penalty', 'notice', 'filing', 'refund', \n",
    "            'portal error', 'ITC', 'composition', 'registration', 'mismatch'\n",
    "        ]\n",
    "        text_lower = text.lower()\n",
    "        for keyword in important_keywords:\n",
    "            if keyword in text_lower and len(key_terms) < max_terms:\n",
    "                key_terms.append(keyword)\n",
    "        \n",
    "        # 4. Financial years\n",
    "        if len(key_terms) < max_terms:\n",
    "            fys = re.findall(r'FY\\s*\\d{4}-?\\d{2,4}', text, re.IGNORECASE)\n",
    "            key_terms.extend(fys[:1])\n",
    "        \n",
    "        return key_terms[:max_terms]\n",
    "    \n",
    "    def _build_focused_query_with_llm(self, query: str, category: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Use classifier LLM to extract focused search query from long text\n",
    "        Reuses existing classifier_llm - no additional model initialization\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            logger.info(\"   â„¹ï¸ No LLM provided, using regex extraction\")\n",
    "            return self._build_focused_query_regex(query, category, None)\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"\"\"Extract a concise web search query (max 30 words) from this GST grievance:\n",
    "\n",
    "Query: {query[:800]}\n",
    "\n",
    "Focus on:\n",
    "- GST form numbers (GSTR-4, GSTR-3A, etc.)\n",
    "- Key issues (late fee, filing, penalty, notice, error)\n",
    "- Notification/section numbers\n",
    "- Important entities only\n",
    "\n",
    "Return ONLY the search query, nothing else. No explanations.\"\"\"\n",
    "            \n",
    "            response = self.llm.invoke(prompt)\n",
    "            focused_query = response.content.strip()[:350]\n",
    "            \n",
    "            logger.info(f\"   ðŸ¤– LLM-optimized query ({len(focused_query)} chars): {focused_query[:80]}...\")\n",
    "            return focused_query\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"âš ï¸ LLM query generation failed: {e}, using regex fallback\")\n",
    "            return self._build_focused_query_regex(query, category, None)\n",
    "    \n",
    "    def _build_focused_query_regex(self, query: str, category: str = None, \n",
    "                                    keywords: List[str] = None, max_length: int = 350) -> str:\n",
    "        \"\"\"\n",
    "        Build a focused query using regex extraction (fallback method)\n",
    "        \"\"\"\n",
    "        # Extract key terms\n",
    "        if keywords:\n",
    "            query_parts = keywords[:5]\n",
    "        else:\n",
    "            query_parts = self._extract_key_terms(query, max_terms=5)\n",
    "        \n",
    "        # Add category context if available\n",
    "        if category and not self.unrestricted:\n",
    "            category_context = {\n",
    "                \"gstr_filing\": \"GST return filing\",\n",
    "                \"penalty_notice\": \"GST penalty late fee notice\",\n",
    "                \"refund\": \"GST refund process\",\n",
    "                \"registration\": \"GST registration\",\n",
    "                \"itc_mismatch\": \"GST ITC mismatch\",\n",
    "                \"eway_bill\": \"GST e-way bill\",\n",
    "                \"portal_error\": \"GST portal error\"\n",
    "            }\n",
    "            context = category_context.get(category, \"GST\")\n",
    "            query_parts.insert(0, context)\n",
    "        \n",
    "        # Build query\n",
    "        focused_query = \" \".join(query_parts)\n",
    "        \n",
    "        # Truncate if still too long\n",
    "        if len(focused_query) > max_length:\n",
    "            focused_query = focused_query[:max_length].rsplit(' ', 1)[0]\n",
    "        \n",
    "        logger.info(f\"   ðŸ” Regex-extracted query ({len(focused_query)} chars): {focused_query[:80]}...\")\n",
    "        return focused_query\n",
    "    \n",
    "    def _build_focused_query_tavily(self, query: str, category: str = None, \n",
    "                                     keywords: List[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Build query for Tavily with 400-char limit\n",
    "        Strategy: Use LLM if available, else fall back to regex\n",
    "        \"\"\"\n",
    "        # Try LLM-based extraction first (preferred)\n",
    "        if self.llm:\n",
    "            return self._build_focused_query_with_llm(query, category)\n",
    "        \n",
    "        # Fall back to regex extraction\n",
    "        return self._build_focused_query_regex(query, category, keywords)\n",
    "    \n",
    "    def _build_query_duckduckgo(self, query: str, category: str = None, \n",
    "                                keywords: List[str] = None) -> str:\n",
    "        \"\"\"Build query for DuckDuckGo (more lenient than Tavily)\"\"\"\n",
    "        if self.unrestricted:\n",
    "            # General web search - truncate to reasonable length\n",
    "            enhanced_query = query[:500]\n",
    "            if keywords:\n",
    "                enhanced_query += \" \" + \" \".join(keywords[:3])\n",
    "        else:\n",
    "            # Restricted to official sites\n",
    "            enhanced_query = f\"{query[:300]} GST India site:gst.gov.in OR site:cbic-gst.gov.in\"\n",
    "        \n",
    "        return enhanced_query\n",
    "    \n",
    "    # --- RETRIEVAL METHODS ---\n",
    "    \n",
    "    def retrieve_tavily(self, query: str, max_results: int = 10) -> List[RetrievalSource]:\n",
    "        \"\"\"\n",
    "        Retrieve using Tavily API\n",
    "        \n",
    "        Args:\n",
    "            query: Optimized search query (already under 400 chars)\n",
    "            max_results: Number of results (default 10)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            search_params = {\n",
    "                \"query\": query,\n",
    "                \"search_depth\": \"advanced\",\n",
    "                \"max_results\": max_results,\n",
    "                \"include_answer\": True,\n",
    "                \"include_raw_content\": False\n",
    "            }\n",
    "            \n",
    "            # Only add domain restrictions if NOT unrestricted\n",
    "            if not self.unrestricted:\n",
    "                search_params[\"include_domains\"] = [\n",
    "                    \"gst.gov.in\",\n",
    "                    \"cbic-gst.gov.in\",\n",
    "                    \"tutorial.gst.gov.in\",\n",
    "                    \"cleartax.in\",\n",
    "                    \"taxguru.in\"\n",
    "                ]\n",
    "                search_params[\"exclude_domains\"] = [\"quora.com\", \"reddit.com\"]\n",
    "            \n",
    "            response = self.client.search(**search_params)\n",
    "            \n",
    "            # Process results\n",
    "            for idx, result in enumerate(response.get('results', [])):\n",
    "                results.append(RetrievalSource(\n",
    "                    source_type=\"web_search\",\n",
    "                    content=f\"**{result.get('title', 'Untitled')}**\\n\\n{result.get('content', '')}\",\n",
    "                    citation=result.get('url', 'Unknown source'),\n",
    "                    relevance_score=result.get('score', 0.8),\n",
    "                    date=None\n",
    "                ))\n",
    "            \n",
    "            # Add direct answer if available\n",
    "            if response.get('answer'):\n",
    "                results.insert(0, RetrievalSource(\n",
    "                    source_type=\"web_answer\",\n",
    "                    content=f\"**AI-Generated Summary:**\\n\\n{response['answer']}\",\n",
    "                    citation=\"Tavily AI Direct Answer\",\n",
    "                    relevance_score=1.0,\n",
    "                    date=None\n",
    "                ))\n",
    "            \n",
    "            logger.info(f\"   ðŸŒ Retrieved {len(results)} results from Tavily\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Tavily search error: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def retrieve_duckduckgo(self, query: str, max_results: int = 10) -> List[RetrievalSource]:\n",
    "        \"\"\"\n",
    "        Retrieve using DuckDuckGo\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            max_results: Number of results (default 10)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            # Execute search\n",
    "            search_results = self.client.run(query)\n",
    "            \n",
    "            # Parse results\n",
    "            if isinstance(search_results, str):\n",
    "                chunks = [chunk.strip() for chunk in search_results.split('\\n\\n') if chunk.strip()]\n",
    "                \n",
    "                for idx, chunk in enumerate(chunks[:max_results]):\n",
    "                    # Extract URL if present\n",
    "                    url_match = re.search(r'\\[(https?://[^\\]]+)\\]', chunk)\n",
    "                    url = url_match.group(1) if url_match else \"DuckDuckGo Search Result\"\n",
    "                    \n",
    "                    # Clean content\n",
    "                    content = re.sub(r'\\[https?://[^\\]]+\\]', '', chunk).strip()\n",
    "                    \n",
    "                    if content:\n",
    "                        results.append(RetrievalSource(\n",
    "                            source_type=\"web_search\",\n",
    "                            content=content,\n",
    "                            citation=url,\n",
    "                            relevance_score=0.8 - (idx * 0.05),\n",
    "                            date=None\n",
    "                        ))\n",
    "            \n",
    "            logger.info(f\"   ðŸŒ Retrieved {len(results)} results from DuckDuckGo\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ DuckDuckGo search error: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def retrieve(self, query: str, category: str = None, \n",
    "                 keywords: List[str] = None, max_results: int = 10) -> List[RetrievalSource]:\n",
    "        \"\"\"\n",
    "        Main retrieval method with intelligent query optimization\n",
    "        \n",
    "        Args:\n",
    "            query: Search query (may be long)\n",
    "            category: GST category (optional)\n",
    "            keywords: Additional keywords (optional)\n",
    "            max_results: Number of results (default 10)\n",
    "        \n",
    "        Returns:\n",
    "            List of RetrievalSource objects\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            logger.warning(\"âš ï¸ No search provider available\")\n",
    "            return []\n",
    "        \n",
    "        # Build optimized query based on provider\n",
    "        if self.provider == \"tavily\":\n",
    "            # Tavily has 400-char limit - use intelligent optimization\n",
    "            enhanced_query = self._build_focused_query_tavily(query, category, keywords)\n",
    "        else:\n",
    "            # DuckDuckGo is more lenient\n",
    "            enhanced_query = self._build_query_duckduckgo(query, category, keywords)\n",
    "        \n",
    "        # Route to provider\n",
    "        if self.provider == \"tavily\":\n",
    "            return self.retrieve_tavily(enhanced_query, max_results)\n",
    "        elif self.provider == \"duckduckgo\":\n",
    "            return self.retrieve_duckduckgo(enhanced_query, max_results)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "# --- TWITTER RETRIEVAL AGENT ---\n",
    "class TwitterRetrievalAgent:\n",
    "    def __init__(self):\n",
    "        self.bearer_token = Config.TWITTER_BEARER_TOKEN\n",
    "        self.client = None\n",
    "        if self.bearer_token and tweepy:\n",
    "            try:\n",
    "                self.client = tweepy.Client(bearer_token=self.bearer_token)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def retrieve(self, keywords: List[str], max_results: int = 10) -> List[RetrievalSource]:\n",
    "        results = []\n",
    "        if not self.client:\n",
    "            logger.warning(\"âš ï¸ Twitter API not configured\")\n",
    "            return results\n",
    "        \n",
    "        try:\n",
    "            query = f\"from:{Config.GSTN_TWITTER.replace('@', '')} ({' OR '.join(keywords)})\"\n",
    "            tweets = self.client.search_recent_tweets(\n",
    "                query=query, max_results=min(max_results, 100),\n",
    "                tweet_fields=[\"created_at\", \"text\"]\n",
    "            )\n",
    "            if tweets.data:\n",
    "                for tweet in tweets.data[:max_results]:\n",
    "                    results.append(RetrievalSource(\n",
    "                        source_type=\"twitter\",\n",
    "                        content=tweet.text,\n",
    "                        citation=f\"@Infosys_GSTN tweet from {tweet.created_at.strftime('%Y-%m-%d')}\",\n",
    "                        relevance_score=0.8,\n",
    "                        date=tweet.created_at.strftime('%Y-%m-%d')\n",
    "                    ))\n",
    "            logger.info(f\"   ðŸ“± Retrieved {len(results)} tweets\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Twitter error: {e}\")\n",
    "        return results\n",
    "\n",
    "# --- NEW: SUB-AGENT 3.4: LLM REASONING RETRIEVAL AGENT ---\n",
    "class LLMReasoningAgent:\n",
    "    \"\"\"\n",
    "    Agent for retrieving reasoning-based analysis from DeepSeek LLM\n",
    "    Provides legal interpretation of core issues based on GST statutes and circulars\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize DeepSeek client\"\"\"\n",
    "        self.api_key = Config.DEEPSEEK_API_KEY\n",
    "        self.model = Config.DEEPSEEK_MODEL\n",
    "        self.client = None\n",
    "        \n",
    "        if self.api_key:\n",
    "            try:\n",
    "                self.client = OpenAI(\n",
    "                    api_key=self.api_key,\n",
    "                    base_url=\"https://api.deepseek.com\"\n",
    "                )\n",
    "                logger.info(\"âœ… DeepSeek LLM client initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ DeepSeek initialization failed: {e}\")\n",
    "                self.client = None\n",
    "        else:\n",
    "            logger.warning(\"âš ï¸ DEEPSEEK_API_KEY not found. LLM reasoning disabled.\")\n",
    "    \n",
    "    def retrieve(self, core_issues: List[CoreIssue], entities: List[ExtractedEntity]) -> List[RetrievalSource]:\n",
    "        \"\"\"\n",
    "        Get reasoning-based analysis from DeepSeek for each core issue\n",
    "        \n",
    "        Args:\n",
    "            core_issues: List of core issues from preprocessing\n",
    "            entities: List of extracted entities for context\n",
    "            \n",
    "        Returns:\n",
    "            List of RetrievalSource objects with LLM reasoning\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        if not self.client:\n",
    "            logger.warning(\"âš ï¸ DeepSeek client not available. Skipping LLM reasoning.\")\n",
    "            return results\n",
    "        \n",
    "        try:\n",
    "            # Build context from entities\n",
    "            entity_context = self._build_entity_context(entities)\n",
    "            \n",
    "            # Process each core issue\n",
    "            for idx, issue in enumerate(core_issues):\n",
    "                logger.info(f\"   ðŸ¤– Querying DeepSeek for issue {idx+1}/{len(core_issues)}...\")\n",
    "                \n",
    "                # Build prompt for this specific issue\n",
    "                prompt = self._build_issue_prompt(issue, entity_context)\n",
    "                \n",
    "                # Call DeepSeek API\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=[\n",
    "                            {\n",
    "                                \"role\": \"system\",\n",
    "                                \"content\": self._get_system_prompt()\n",
    "                            },\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": prompt\n",
    "                            }\n",
    "                        ],\n",
    "                        temperature=0.3,  # Balanced between creativity and consistency\n",
    "                        max_tokens=1000,\n",
    "                        stream=False\n",
    "                    )\n",
    "                    \n",
    "                    reasoning = response.choices[0].message.content\n",
    "                    \n",
    "                    # Add to results\n",
    "                    results.append(RetrievalSource(\n",
    "                        source_type=\"llm_reasoning\",\n",
    "                        content=f\"**Issue {idx+1}: {issue.issue_text}**\\n\\n{reasoning}\",\n",
    "                        citation=f\"DeepSeek Legal Analysis (Issue {idx+1})\",\n",
    "                        relevance_score=0.95,  # High relevance as it's targeted analysis\n",
    "                        date=None\n",
    "                    ))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"âŒ DeepSeek API error for issue {idx+1}: {e}\")\n",
    "                    # Add error placeholder\n",
    "                    results.append(RetrievalSource(\n",
    "                        source_type=\"llm_reasoning\",\n",
    "                        content=f\"**Issue {idx+1}: {issue.issue_text}**\\n\\nUnable to retrieve reasoning: {str(e)}\",\n",
    "                        citation=f\"DeepSeek Analysis (Error)\",\n",
    "                        relevance_score=0.0,\n",
    "                        date=None\n",
    "                    ))\n",
    "            \n",
    "            logger.info(f\"   ðŸ¤– Retrieved {len(results)} LLM reasoning results from DeepSeek\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ LLM reasoning retrieval error: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_system_prompt(self) -> str:\n",
    "        \"\"\"Get the system prompt for DeepSeek\"\"\"\n",
    "        return \"\"\"You are an expert GST (Goods and Services Tax) legal advisor for India with deep knowledge of:\n",
    "- Central GST Act, 2017 and State GST Acts\n",
    "- GST Rules and Regulations\n",
    "- CBIC (Central Board of Indirect Taxes and Customs) notifications, circulars, and orders\n",
    "- GST Council decisions and press releases\n",
    "- GSTN (Goods and Services Tax Network) guidelines\n",
    "- Case laws and tribunal decisions related to GST\n",
    "\n",
    "Your role is to provide accurate, legally sound analysis of GST grievances based on:\n",
    "1. Relevant sections of CGST/SGST/IGST Acts\n",
    "2. Applicable rules and notifications\n",
    "3. Official circulars and clarifications\n",
    "4. Precedents from case law (if relevant)\n",
    "5. If user quotes a statute or notificaton, do check if the same has been amended in future.\n",
    "\n",
    "Provide clear, actionable guidance while citing specific legal provisions. If unsure about any aspect, explicitly state the uncertainty and recommend verification from official sources or professional consultation.\"\"\"\n",
    "\n",
    "    def _build_entity_context(self, entities: List[ExtractedEntity]) -> str:\n",
    "        \"\"\"Build context string from extracted entities\"\"\"\n",
    "        if not entities:\n",
    "            return \"No specific entities identified.\"\n",
    "        \n",
    "        entity_groups = {}\n",
    "        for entity in entities:\n",
    "            entity_type = entity.entity_type\n",
    "            if entity_type not in entity_groups:\n",
    "                entity_groups[entity_type] = []\n",
    "            entity_groups[entity_type].append(entity.value)\n",
    "        \n",
    "        context_parts = []\n",
    "        for entity_type, values in entity_groups.items():\n",
    "            context_parts.append(f\"- {entity_type}: {', '.join(values)}\")\n",
    "        \n",
    "        return \"**Context:**\\n\" + \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _build_issue_prompt(self, issue: CoreIssue, entity_context: str) -> str:\n",
    "        \"\"\"Build prompt for a specific issue\"\"\"\n",
    "        return f\"\"\"Analyze the following GST grievance issue and provide expert legal guidance.\n",
    "\n",
    "**Issue:**\n",
    "{issue.issue_text}\n",
    "\n",
    "**Keywords:** {', '.join(issue.keywords)}\n",
    "\n",
    "{entity_context}\n",
    "\n",
    "**Instructions:**\n",
    "1. Identify applicable GST provisions (Acts, Rules, Notifications, Circulars)\n",
    "2. Provide legal interpretation based on current GST law\n",
    "3. Explain the correct procedure or resolution\n",
    "4. Cite specific sections, rules, or notification numbers where relevant\n",
    "5. Highlight any recent updates or clarifications from CBIC/GSTN\n",
    "6. If the issue involves misconceptions, clarify them with legal basis\n",
    "7. Keep the response concise (max 300 words) but comprehensive\n",
    "\n",
    "**Response Format:**\n",
    "- Legal Basis: [Cite specific provisions]\n",
    "- Analysis: [Your legal interpretation]\n",
    "- Resolution: [Clear guidance on what the taxpayer should do]\n",
    "- Important Notes: [Any warnings, deadlines, or critical points]\"\"\"\n",
    "\n",
    "# --- RETRIEVAL ORCHESTRATOR ---\n",
    "class RetrievalOrchestrator:\n",
    "    \"\"\"Orchestrates retrieval from all sources (4 agents)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.twitter_agent = TwitterRetrievalAgent()\n",
    "        \n",
    "        self.local_agent = LocalRetrievalAgent(\n",
    "            embeddings=local_embeddings,\n",
    "            kb_folder=\"./\",\n",
    "            enable_graph=True\n",
    "        )\n",
    "        \n",
    "        # Pass classifier_llm for query optimization\n",
    "        self.web_agent = WebRetrievalAgent(\n",
    "            provider=\"auto\",\n",
    "            unrestricted=Config.WEB_SEARCH_UNRESTRICTED,\n",
    "            llm=classifier_llm  # Reuse classifier LLM\n",
    "        )\n",
    "        \n",
    "        self.llm_agent = LLMReasoningAgent()\n",
    "        \n",
    "        logger.info(\"âœ… Retrieval orchestrator initialized\")\n",
    "        stats = self.local_agent.get_stats()\n",
    "        logger.info(f\"   ðŸ“Š Local KB:\")\n",
    "        logger.info(f\"      Files: {stats['source_files']}\")\n",
    "        logger.info(f\"      Chunks: {stats['document_count']}\")\n",
    "        logger.info(f\"      Size: {stats['index_size_mb']:.2f} MB\")\n",
    "        if stats['graph_enabled']:\n",
    "            logger.info(f\"      Graph: {stats['graph_nodes']} nodes, {stats['graph_edges']} edges\")\n",
    "        logger.info(f\"   ðŸŒ Web search: {self.web_agent.provider} (LLM optimization: {'enabled' if self.web_agent.llm else 'disabled'})\")\n",
    "        logger.info(f\"   ðŸ¤– LLM reasoning: {'enabled' if self.llm_agent.client else 'disabled'}\")\n",
    "    \n",
    "    \n",
    "    def process(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"\n",
    "        THIS METHOD WAS MISSING - Process retrieval from all sources\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"ðŸ”„ Agent 3: Retrieving from multiple sources (4 agents)...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            preprocessing = state[\"preprocessing_output\"]\n",
    "            classification = state[\"classification_output\"]\n",
    "            \n",
    "            # Extract keywords from all core issues\n",
    "            all_keywords = list(set([\n",
    "                kw for issue in preprocessing.core_issues for kw in issue.keywords\n",
    "            ]))\n",
    "            \n",
    "            # Build combined query\n",
    "            combined_query = \" \".join([issue.issue_text for issue in preprocessing.core_issues])\n",
    "            \n",
    "            # 1. Twitter (optional, real-time)\n",
    "            twitter_results = self.twitter_agent.retrieve(all_keywords, Config.MAX_TWITTER_RESULTS)\n",
    "            \n",
    "            # 2. Local KB (FAISS vector store)\n",
    "            local_results = self.local_agent.retrieve(\n",
    "                query=combined_query,\n",
    "                k=Config.MAX_LOCAL_RESULTS,\n",
    "                filter_category=None,\n",
    "                use_graph=True  # Enable graph-enhanced retrieval\n",
    "            )\n",
    "            \n",
    "            # 3. Web search\n",
    "            web_results = self.web_agent.retrieve(\n",
    "                query=combined_query,\n",
    "                category=classification.primary_category if not Config.WEB_SEARCH_UNRESTRICTED else None,\n",
    "                keywords=all_keywords[:5],\n",
    "                max_results=Config.MAX_WEB_RESULTS\n",
    "            )\n",
    "            \n",
    "            # 4. LLM Reasoning (DeepSeek)\n",
    "            llm_reasoning = self.llm_agent.retrieve(\n",
    "                core_issues=preprocessing.core_issues,\n",
    "                entities=preprocessing.entities\n",
    "            )\n",
    "            \n",
    "            # Aggregate results\n",
    "            retrieval_output = RetrievalOutput(\n",
    "                twitter_results=twitter_results,\n",
    "                local_results=local_results,\n",
    "                web_results=web_results,\n",
    "                llm_reasoning=llm_reasoning,\n",
    "                total_sources=len(twitter_results) + len(local_results) + len(web_results) + len(llm_reasoning),\n",
    "                retrieval_time=time.time() - start_time\n",
    "            )\n",
    "            \n",
    "            state[\"retrieval_output\"] = retrieval_output\n",
    "            \n",
    "            logger.info(f\"âœ… Retrieved {retrieval_output.total_sources} sources\")\n",
    "            logger.info(f\"   ðŸ“± Twitter: {len(twitter_results)}\")\n",
    "            logger.info(f\"   ðŸ“– Local KB: {len(local_results)}\")\n",
    "            logger.info(f\"   ðŸŒ Web: {len(web_results)}\")\n",
    "            logger.info(f\"   ðŸ¤– LLM Reasoning: {len(llm_reasoning)}\")\n",
    "            \n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Retrieval error: {e}\")\n",
    "            state[\"errors\"].append(f\"Retrieval error: {str(e)}\")\n",
    "            return state\n",
    "\n",
    "# --- RESOLVER AND RESPONSE AGENTS ---\n",
    "class ResolverAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.parser = JsonOutputParser(pydantic_object=ResolverOutput)\n",
    "\n",
    "    def process(self, state: AgentState) -> AgentState:\n",
    "        try:\n",
    "            logger.info(\"ðŸ”„ Agent 4: Resolving with Gemini 2.5 Flash...\")\n",
    "            preprocessing = state[\"preprocessing_output\"]\n",
    "            retrieval = state[\"retrieval_output\"]\n",
    "            \n",
    "            # UPDATED: Include LLM reasoning in prompt\n",
    "            prompt = f\"\"\"\n",
    "Resolve these GST issues using provided sources:\n",
    "\n",
    "Core Issues: {json.dumps([i.model_dump() for i in preprocessing.core_issues])}\n",
    "\n",
    "**Retrieved Sources:**\n",
    "\n",
    "Twitter Updates: {[s.content for s in retrieval.twitter_results[:3]]}\n",
    "\n",
    "Local Knowledge Base: {[s.content for s in retrieval.local_results[:5]]}\n",
    "\n",
    "Web Search Results: {[s.content for s in retrieval.web_results[:5]]}\n",
    "\n",
    "DeepSeek Legal Reasoning: {[s.content for s in retrieval.llm_reasoning]}\n",
    "\n",
    "**Instructions:**\n",
    "- Provide accurate, actionable resolutions for each issue\n",
    "- Cross-validate information across all sources\n",
    "- Give priority to LLM reasoning for legal interpretation\n",
    "- Cite sources appropriately\n",
    "- Min confidence: 95. If uncertain, set resolution to null with reason\n",
    "- Return JSON matching ResolverOutput schema\n",
    "\n",
    "{self.parser.get_format_instructions()}\n",
    "\"\"\"\n",
    "            \n",
    "            response = self.llm.invoke(prompt)\n",
    "            resolver_output = ResolverOutput(**json.loads(response.content))\n",
    "            state[\"resolver_output\"] = resolver_output\n",
    "            logger.info(f\"âœ… Confidence: {resolver_output.overall_confidence}%\")\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Resolver error: {e}\")\n",
    "            state[\"resolver_output\"] = ResolverOutput(\n",
    "                resolutions=[], overall_confidence=0, requires_escalation=True\n",
    "            )\n",
    "            return state\n",
    "\n",
    "\n",
    "class ResponseGenerationAgent:\n",
    "    def process(self, state: AgentState) -> AgentState:\n",
    "        try:\n",
    "            logger.info(\"ðŸ”„ Agent 5: Generating response...\")\n",
    "            resolver = state[\"resolver_output\"]\n",
    "            retrieval = state[\"retrieval_output\"]\n",
    "            \n",
    "            parts = []\n",
    "            for res in resolver.resolutions:\n",
    "                if res.resolution:\n",
    "                    parts.append(f\"**{res.issue}**\\n{res.resolution}\")\n",
    "                else:\n",
    "                    parts.append(f\"**{res.issue}**\\nâš ï¸ Unable to answer. {res.reason_for_null}\")\n",
    "            \n",
    "            direct_answer = \"\\n\\n\".join(parts) if parts else \"Manual review recommended.\"\n",
    "            \n",
    "            recent_updates = None\n",
    "            if retrieval.twitter_results:\n",
    "                recent_updates = \"Recent GSTN Updates:\\n\" + \"\\n\".join([\n",
    "                    f\"- {s.content} ({s.citation})\" for s in retrieval.twitter_results[:3]\n",
    "                ])\n",
    "            \n",
    "            final_response = FinalResponse(\n",
    "                direct_answer=direct_answer,\n",
    "                detailed_explanation=None,\n",
    "                legal_basis=None,\n",
    "                recent_updates=recent_updates,\n",
    "                additional_resources=[\n",
    "                    \"GST Portal: https://www.gst.gov.in\",\n",
    "                    \"GSTN Tutorials: https://tutorial.gst.gov.in\",\n",
    "                    \"CBIC GST: https://cbic-gst.gov.in\"\n",
    "                ],\n",
    "                confidence_score=resolver.overall_confidence,\n",
    "                requires_manual_review=resolver.requires_escalation\n",
    "            )\n",
    "            \n",
    "            state[\"final_response\"] = final_response\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Response generation error: {e}\")\n",
    "            return state\n",
    "\n",
    "# --- WORKFLOW ---\n",
    "def should_escalate(state: AgentState) -> str:\n",
    "    if state.get(\"escalation_requested\") or (state.get(\"final_response\") and state[\"final_response\"].requires_manual_review):\n",
    "        return \"escalate\"\n",
    "    return \"complete\"\n",
    "\n",
    "def handle_escalation(state: AgentState) -> AgentState:\n",
    "    logger.info(\"ðŸ”„ Agent 6: Escalating...\")\n",
    "    if state[\"final_response\"]:\n",
    "        state[\"final_response\"].direct_answer = \"Escalated to GST expert team.\\n\\n\" + state[\"final_response\"].direct_answer\n",
    "    return state\n",
    "\n",
    "def create_workflow():\n",
    "    preprocessing_agent = PreprocessingAgent(preprocessor_llm)\n",
    "    classification_agent = ClassificationAgent(classifier_llm)\n",
    "    retrieval_orchestrator = RetrievalOrchestrator()\n",
    "    resolver_agent = ResolverAgent(resolver_llm)\n",
    "    response_agent = ResponseGenerationAgent()\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.add_node(\"preprocessing\", preprocessing_agent.process)\n",
    "    workflow.add_node(\"classification\", classification_agent.process)\n",
    "    workflow.add_node(\"retrieval\", retrieval_orchestrator.process)\n",
    "    workflow.add_node(\"resolver\", resolver_agent.process)\n",
    "    workflow.add_node(\"response_generation\", response_agent.process)\n",
    "    workflow.add_node(\"escalation\", handle_escalation)\n",
    "\n",
    "    workflow.set_entry_point(\"preprocessing\")\n",
    "    workflow.add_edge(\"preprocessing\", \"classification\")\n",
    "    workflow.add_edge(\"classification\", \"retrieval\")\n",
    "    workflow.add_edge(\"retrieval\", \"resolver\")\n",
    "    workflow.add_edge(\"resolver\", \"response_generation\")\n",
    "    workflow.add_conditional_edges(\"response_generation\", should_escalate, {\n",
    "        \"escalate\": \"escalation\", \"complete\": END\n",
    "    })\n",
    "    workflow.add_edge(\"escalation\", END)\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "# --- MAIN FUNCTION ---\n",
    "def process_gst_grievance(query: str, session_id: str = None) -> Dict[str, Any]:\n",
    "    initial_state = AgentState(\n",
    "        user_query=query,\n",
    "        session_id=session_id or f\"session_{int(time.time())}\",\n",
    "        conversation_history=[],\n",
    "        preprocessing_output=None,\n",
    "        classification_output=None,\n",
    "        retrieval_output=None,\n",
    "        resolver_output=None,\n",
    "        final_response=None,\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        processing_time=0.0,\n",
    "        errors=[],\n",
    "        feedback_received=None,\n",
    "        iteration_count=0,\n",
    "        escalation_requested=False\n",
    "    )\n",
    "\n",
    "    workflow_app = create_workflow()\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"ðŸš€ Starting GST Grievance Resolution\")\n",
    "    logger.info(f\"ðŸ“ Query: {query}\")\n",
    "    logger.info(\"=\" * 80)\n",
    "\n",
    "    final_state = workflow_app.invoke(initial_state)\n",
    "\n",
    "    result = {\n",
    "        \"session_id\": final_state[\"session_id\"],\n",
    "        \"query\": query,\n",
    "        \"response\": final_state[\"final_response\"].model_dump() if final_state[\"final_response\"] else None,\n",
    "        \"preprocessing\": final_state[\"preprocessing_output\"].model_dump() if final_state[\"preprocessing_output\"] else None,\n",
    "        \"classification\": final_state[\"classification_output\"].model_dump() if final_state[\"classification_output\"] else None,\n",
    "        \"retrieval_stats\": {\n",
    "            \"total_sources\": final_state[\"retrieval_output\"].total_sources if final_state[\"retrieval_output\"] else 0,\n",
    "            \"retrieval_time\": final_state[\"retrieval_output\"].retrieval_time if final_state[\"retrieval_output\"] else 0,\n",
    "            \"local_count\": len(final_state[\"retrieval_output\"].local_results) if final_state[\"retrieval_output\"] else 0,\n",
    "            \"web_count\": len(final_state[\"retrieval_output\"].web_results) if final_state[\"retrieval_output\"] else 0,\n",
    "            \"twitter_count\": len(final_state[\"retrieval_output\"].twitter_results) if final_state[\"retrieval_output\"] else 0,\n",
    "            \"llm_count\": len(final_state[\"retrieval_output\"].llm_reasoning) if final_state[\"retrieval_output\"] else 0,\n",
    "        },\n",
    "        \"resolution_stats\": {\n",
    "            \"overall_confidence\": final_state[\"resolver_output\"].overall_confidence if final_state[\"resolver_output\"] else 0,\n",
    "            \"requires_escalation\": final_state[\"resolver_output\"].requires_escalation if final_state[\"resolver_output\"] else True\n",
    "        },\n",
    "        \"processing_time\": final_state[\"processing_time\"],\n",
    "        \"errors\": final_state[\"errors\"]\n",
    "    }\n",
    "\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"âœ… Complete. Time: {final_state['processing_time']:.2f}s\")\n",
    "    logger.info(f\"ðŸŽ¯ Confidence: {result['resolution_stats']['overall_confidence']}%\")\n",
    "    logger.info(\"=\" * 80)\n",
    "\n",
    "    return result\n",
    "\n",
    "# --- DISPLAY FUNCTION ---\n",
    "def display_result(result: Dict[str, Any]):\n",
    "    if not result[\"response\"]:\n",
    "        display(Markdown(\"## âŒ Error: No response generated\"))\n",
    "        return\n",
    "\n",
    "    response = result[\"response\"]\n",
    "    display(Markdown(f\"# ðŸŽ¯ GST Grievance Response\"))\n",
    "    display(Markdown(f\"**Session:** {result['session_id']}\"))\n",
    "    display(Markdown(f\"**Time:** {result['processing_time']:.2f}s\"))\n",
    "    display(Markdown(f\"**Confidence:** {response['confidence_score']}%\"))\n",
    "    display(Markdown(f\"**Status:** {'âš ï¸ Manual review' if response['requires_manual_review'] else 'âœ… Resolved'}\"))\n",
    "    \n",
    "    # Display retrieval stats\n",
    "    stats = result['retrieval_stats']\n",
    "    display(Markdown(f\"**Sources:** {stats['total_sources']} (Local: {stats['local_count']}, Web: {stats['web_count']}, Twitter: {stats['twitter_count']}, LLM: {stats.get('llm_count', 0)})\"))\n",
    "    \n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"## ðŸ“ Answer\"))\n",
    "    display(Markdown(response[\"direct_answer\"]))\n",
    "    \n",
    "    if response[\"recent_updates\"]:\n",
    "        display(Markdown(\"## ðŸ“± Recent Updates\"))\n",
    "        display(Markdown(response[\"recent_updates\"]))\n",
    "    \n",
    "    display(Markdown(\"## ðŸ”— Resources\"))\n",
    "    for resource in response[\"additional_resources\"]:\n",
    "        display(Markdown(f\"- {resource}\"))\n",
    "\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "def add_to_knowledge_base(documents: List[dict]):\n",
    "    \"\"\"Add documents to local knowledge base\"\"\"\n",
    "    retrieval_orchestrator = RetrievalOrchestrator()\n",
    "    retrieval_orchestrator.local_agent.add_documents(documents)\n",
    "\n",
    "def get_kb_stats():\n",
    "    \"\"\"Get knowledge base statistics\"\"\"\n",
    "    retrieval_orchestrator = RetrievalOrchestrator()\n",
    "    return retrieval_orchestrator.local_agent.get_stats()\n",
    "\n",
    "# --- USAGE ---\n",
    "print(\"âœ… GST Grievance Resolution System Loaded!\")\n",
    "print(\"\\nðŸ“š System Features:\")\n",
    "print(\"   â€¢ Local embeddings (MPS/CUDA accelerated)\")\n",
    "print(\"   â€¢ Persistent FAISS vector store\")\n",
    "print(\"   â€¢ Web search (Tavily/DuckDuckGo)\")\n",
    "print(\"   â€¢ Twitter real-time updates\")\n",
    "print(\"\\nðŸ’¡ Usage:\")\n",
    "print('   result = process_gst_grievance(\"Your GST query here\")')\n",
    "print('   display_result(result)')\n",
    "print('\\nðŸ”§ Utilities:')\n",
    "print('   stats = get_kb_stats()  # View knowledge base stats')\n",
    "print('   add_to_knowledge_base([{...}])  # Add documents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13577aca-011f-4841-bb65-3f5d7e3ec633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e01c69-6bbe-4678-a368-03ad20ee9f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
